{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Zendesk articles for search\n",
    "\n",
    "This notebook shows how we prepared a dataset of Wikipedia articles for search, used in [Question_answering_using_embeddings.ipynb](Question_answering_using_embeddings.ipynb).\n",
    "\n",
    "Procedure:\n",
    "\n",
    "0. Prerequisites: Import libraries, set API key (if needed)\n",
    "1. Collect: We download a few hundred Wikipedia articles about the 2022 Olympics\n",
    "2. Chunk: Documents are split into short, semi-self-contained sections to be embedded\n",
    "3. Embed: Each section is embedded with the OpenAI API\n",
    "4. Store: Embeddings are saved in a CSV file (for large datasets, use a vector database)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Prerequisites\n",
    "\n",
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import mwclient  # for downloading example Wikipedia articles\n",
    "import mwparserfromhell  # for splitting Wikipedia articles into sections\n",
    "import openai  # for generating embeddings\n",
    "import numpy as np  # for arrays to store embeddings\n",
    "import pandas as pd  # for DataFrames to store article sections and embeddings\n",
    "import re  # for cutting <ref> links out of Wikipedia articles\n",
    "import tiktoken  # for counting tokens\n",
    "from datetime import datetime\n",
    "# Import the Zenpy Class\n",
    "from zenpy import Zenpy\n",
    "from zenpy.lib.api_objects import Ticket\n",
    "from pprint import pprint\n",
    "from scipy import spatial  # for calculating vector similarities for search\n",
    "import typing  # for type hints\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install any missing libraries with `pip install` in your terminal. E.g.,\n",
    "\n",
    "```zsh\n",
    "pip install openai\n",
    "```\n",
    "\n",
    "(You can also do this in a notebook cell with `!pip install openai`.)\n",
    "\n",
    "If you install any libraries, be sure to restart the notebook kernel."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set API key (if needed)\n",
    "\n",
    "Note that the OpenAI library will try to read your API key from the `OPENAI_API_KEY` environment variable. If you haven't already, set this environment variable by following [these instructions](https://help.openai.com/en/articles/5112595-best-practices-for-api-key-safety)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "openai.organization = \"org-C15lzQ0mQYcGkjGrpiBPk2Hb\"\n",
    "openai.api_key = \"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\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "MAX_INPUT_TOKENS = 8191\n",
    "COMPLETIONS_MODEL = \"text-davinci-003\"\n",
    "CHAT_COMPLETIONS_MODEL=\"gpt-3.5-turbo\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Configure Zendesk API config\n",
    "\n",
    "The Zendesk API is configured in the [Zendesk dashboard](https://app.zendesk.com/hc/en-us/articles/360001111134-Zendesk-API-Configuration)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Zenpy accepts an API token\n",
    "creds = {\n",
    "    \"email\": \"chisom@exam-genius.com\",\n",
    "    \"token\": \"1ASu216KqW6p0BHrBIOSAYaBlax2NmHvRu5rCAAk\",\n",
    "    \"subdomain\": \"omnicentra\",\n",
    "}\n",
    "\n",
    "# Default\n",
    "zenpy_client = Zenpy(**creds)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Collect articles\n",
    "\n",
    "In this example, we'll download a few hundred Wikipedia articles related to the 2022 Winter Olympics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date_string():\n",
    "    return datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "\n",
    "def fetch_zendesk_sections():\n",
    "    sections = []\n",
    "    for section in zenpy_client.help_center.sections():\n",
    "        if section.name == \"IT Queries\":\n",
    "            section.name = \"IT\"\n",
    "        else:\n",
    "            section.name = \"HR\"\n",
    "        sections.append(section)\n",
    "        pass\n",
    "    return sections\n",
    "\n",
    "\n",
    "def fetch_all_zendesk_articles():\n",
    "    articles = zenpy_client.help_center.articles()\n",
    "    for article in articles:\n",
    "        pprint(article)\n",
    "        pass\n",
    "    return articles\n",
    "\n",
    "\n",
    "def fetch_zendesk_articles_by_section(sections):\n",
    "    my_articles = []\n",
    "    for _section in sections:\n",
    "        category = \"IT\" if _section.name == \"IT\" else \"HR\"\n",
    "        print(f\"Searching for articles in section {_section.name}\")\n",
    "        articles = zenpy_client.help_center.sections.articles(section=_section)\n",
    "        print(f\"Found {len(articles)} articles in section {_section}\")\n",
    "        for article in articles:\n",
    "            # pprint(\"--------------------------------------------------------------------------------------------------\")\n",
    "            my_articles.append((article.title, article.body, category))\n",
    "            pass\n",
    "    return my_articles"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Fetch All Article sections"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "article_sections = fetch_zendesk_sections()\n",
    "print(article_sections)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for section in article_sections:\n",
    "    print(section.name)\n",
    "    pass"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Fetch all articles for each section"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "articles = fetch_zendesk_articles_by_section(article_sections)\n",
    "len(articles)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_txt_knowledge_base(articles, path: str):\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "\n",
    "    with open(f\"{path}/base.txt\", \"w\") as file:\n",
    "        for article in articles:\n",
    "            file.write(article[0] + \"\\n\" + article[1] + \"\\n\" + article[2] + \"\\n\\n\")\n",
    "            pass\n",
    "    return True"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "create_txt_knowledge_base(articles, f\"knowledge_base/{get_date_string()}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chunk documents\n",
    "\n",
    "Now that we have our reference documents, we need to prepare them for search.\n",
    "\n",
    "Because GPT can only read a limited amount of text at once, we'll split each document into chunks short enough to be read.\n",
    "\n",
    "For this specific example on Wikipedia articles, we'll:\n",
    "- Remove all html syntax tags (e.g., \\<ref>\\, \\<div>\\), whitespace, and super short sections\n",
    "- Clean up the text by removing reference tags (e.g., <ref>), whitespace, and super short sections\n",
    "- Split each article into sections\n",
    "- Prepend titles and subtitles to each section's text, to help GPT understand the context\n",
    "- If a section is long (say, > 1,600 tokens), we'll recursively split it into smaller sections, trying to split along semantic boundaries like paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens_from_text(string: str, encoding_name: str = \"cl100k_base\") -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def clean_up_text(articles):\n",
    "    cleaned_articles = []\n",
    "    for title, body, category in articles:\n",
    "        cleaned_body = BeautifulSoup(body, \"html.parser\").get_text()\n",
    "        if num_tokens_from_text(title.strip() + cleaned_body.strip()) > MAX_INPUT_TOKENS:\n",
    "            left = body[:MAX_INPUT_TOKENS]\n",
    "            right = body[MAX_INPUT_TOKENS:]\n",
    "            cleaned_articles.append((title, left, category))\n",
    "            cleaned_articles.append((title, right, category))\n",
    "        else:\n",
    "            cleaned_articles.append((title, cleaned_body, category))\n",
    "    pass\n",
    "    return cleaned_articles\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "CLEANED_ARTICLES = clean_up_text(articles)\n",
    "np.array(CLEANED_ARTICLES).shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print example data\n",
    "for article in CLEANED_ARTICLES[:5]:\n",
    "    print(article[0])\n",
    "    display(article[1][:77])\n",
    "    print(article[2])\n",
    "    print(\"-\"*50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll recursively split long sections into smaller sections.\n",
    "\n",
    "There's no perfect recipe for splitting text into sections.\n",
    "\n",
    "Some tradeoffs include:\n",
    "- Longer sections may be better for questions that require more context\n",
    "- Longer sections may be worse for retrieval, as they may have more topics muddled together\n",
    "- Shorter sections are better for reducing costs (which are proportional to the number of tokens)\n",
    "- Shorter sections allow more sections to be retrieved, which may help with recall\n",
    "- Overlapping sections may help prevent answers from being cut by section boundaries\n",
    "\n",
    "Here, we'll use a simple approach and limit sections to 1,600 tokens each, recursively halving any sections that are too long. To avoid cutting in the middle of useful sentences, we'll split along paragraph boundaries when possible."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Embed document chunks\n",
    "\n",
    "Now that we've split our library into shorter self-contained strings, we can compute embeddings for each.\n",
    "\n",
    "(For large embedding jobs, use a script like [api_request_parallel_processor.py](api_request_parallel_processor.py) to parallelize requests while throttling to stay under rate limits.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate embeddings\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002\"  # OpenAI's best embeddings as of Apr 2023\n",
    "BATCH_SIZE = 1000  # you can submit up to 2048 embedding inputs per request\n",
    "\n",
    "\n",
    "def calculate_embeddings(articles):\n",
    "    titles = []\n",
    "    content = []\n",
    "    categories = []\n",
    "    embeddings = []\n",
    "    for batch_start in range(0, len(articles), BATCH_SIZE):\n",
    "        batch_end = batch_start + BATCH_SIZE\n",
    "        batch = articles[batch_start:batch_end]\n",
    "        titles.extend([article[0] for article in batch])\n",
    "        content.extend([article[1] for article in batch])\n",
    "        categories.extend([article[2] for article in batch])\n",
    "        batch_text = [title + \" \" + body for title, body, category in batch]\n",
    "        print(f\"Batch {batch_start} to {batch_end - 1}\")\n",
    "        response = openai.Embedding.create(model=EMBEDDING_MODEL, input=batch_text)\n",
    "        for i, be in enumerate(response[\"data\"]):\n",
    "            assert i == be[\"index\"]  # double check embeddings are in same order as input\n",
    "        batch_embeddings = [e[\"embedding\"] for e in response[\"data\"]]\n",
    "        embeddings.extend(batch_embeddings)\n",
    "\n",
    "    return pd.DataFrame({\"titles\": titles, \"content\": content, \"categories\": categories, \"embedding\": embeddings}), embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "DF, EMBEDDINGS = calculate_embeddings(CLEANED_ARTICLES)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Store document chunks and embeddings\n",
    "\n",
    "Because this example only uses a few thousand strings, we'll store them in a CSV file.\n",
    "\n",
    "(For larger datasets, use a vector database, which will be more performant.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save document chunks and embeddings\n",
    "def save_dataframe_to_csv(df: pd.DataFrame, path: str, filename: str):\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "        print(f\"Created {path}\")\n",
    "    df.to_csv(f\"{path}/{filename}\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "save_dataframe_to_csv(DF, f\"data/{get_date_string()}\", \"zendesk_vector_embeddings.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Store embeddings in Pinecone database"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Initialise pinecone client with valid API key and environment\n",
    "import pinecone\n",
    "\n",
    "pinecone.init(api_key=\"50f995ae-f134-4a60-8aba-edf67c153790\", environment=\"us-west1-gcp-free\")\n",
    "# Connect to the \"Alfred\" index\n",
    "index = pinecone.Index(\"alfred\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Insert the vector embeddings into the index\n",
    "from tqdm.auto import tqdm  # this is our progress bar\n",
    "\n",
    "\n",
    "def store_embeddings_into_pinecone(embeddings: np.ndarray, index: pinecone.Index):\n",
    "    batch_size = 32  # process everything in batches of 32\n",
    "    for i in tqdm(range(0, len(DF), batch_size)):\n",
    "        i_end = min(i + batch_size, len(DF))\n",
    "        batch = DF[i: i + batch_size]\n",
    "        embeddings_batch = batch[\"embedding\"]\n",
    "        ids_batch = [str(n) for n in range(i, i_end)]\n",
    "        # prep metadata and upsert batch\n",
    "        meta = [{'title': titles, \"content\": content, \"category\": categories} for titles, content, categories, embeddings in batch.to_numpy()]\n",
    "        to_upsert = zip(ids_batch, embeddings_batch, meta)\n",
    "        print(to_upsert)\n",
    "        index.upsert(vectors=list(to_upsert))\n",
    "        # upsert to Pinecone"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "store_embeddings_into_pinecone(EMBEDDINGS, index)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "DF.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_queries():\n",
    "    queries = []\n",
    "    # removing the new line characters\n",
    "    with open('test/queries.txt') as f:\n",
    "        lines = [line.rstrip() for line in f]\n",
    "        for line in lines:\n",
    "            queries.append(line)\n",
    "    return queries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6. Create VE for test query and retrieve embeddings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from openai.embeddings_utils import cosine_similarity, get_embedding\n",
    "\n",
    "# search function\n",
    "def strings_ranked_by_relatedness(\n",
    "        query: str,\n",
    "        df: pd.DataFrame,\n",
    "        relatedness_fn=lambda x, y: cosine_similarity(x, y),\n",
    "        top_n: int = 100\n",
    ") -> tuple[list[str], list[float]]:\n",
    "    \"\"\"Returns a list of strings and relatednesses, sorted from most related to least.\"\"\"\n",
    "    question_vector = get_embedding(query, EMBEDDING_MODEL)\n",
    "    strings_and_relatednesses = [\n",
    "        (row[\"content\"], relatedness_fn(row[\"embedding\"], question_vector), row[\"embedding\"])\n",
    "        for i, row in df.iterrows()\n",
    "    ]\n",
    "    strings_and_relatednesses.sort(key=lambda x: x[1], reverse=True)\n",
    "    strings, relatednesses, embedding = zip(*strings_and_relatednesses)\n",
    "    return strings[:top_n], relatednesses[:top_n], embedding"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_similarities(query: typing.List[str], df: pd.DataFrame) -> pd.DataFrame:\n",
    "    SCORES = []\n",
    "    ANSWERS = []\n",
    "    EMBEDDINGS = []\n",
    "    strings, relatednesses, embeddings = strings_ranked_by_relatedness(query, df, top_n=3)\n",
    "    for string, relatedness, embedding in zip(strings, relatednesses, embeddings):\n",
    "        ANSWERS.append(string)\n",
    "        SCORES.append(\"%.3f\" % relatedness)\n",
    "        EMBEDDINGS.append(embedding)\n",
    "\n",
    "    results = pd.DataFrame({\"answers\": ANSWERS, \"match_scores\": SCORES, \"embeddings\": EMBEDDINGS})\n",
    "    return results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 7. Combine all top n answers into one chunk of text to use as knowledge base context for GPT"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def generate_context_array(results: pd.DataFrame) -> str:\n",
    "    context_array = []\n",
    "    for i, row in results.iterrows():\n",
    "        context_array.append(row.answers)\n",
    "\n",
    "    context = \"\\n\".join(context_array)\n",
    "    return context"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 8. Use GPT3 model to generate user-friendly answers to the query"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "def generate_gpt_opt_response(record: pd.Series, category: Literal[\"IT\", \"HR\"], company: str=\"Omnicentra\", description: str=\"an AI software company\"):\n",
    "    question = record.question\n",
    "    context = record.top_answer\n",
    "    prompt = f\"\"\"Name: Alfred\n",
    "\n",
    "\"Answer the following question by rephrasing the context below\"\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "You are an AI-powered assistant designed to help employees with {category} questions at {company}. You have been programmed to provide fast and accurate solutions to their inquiries. As an AI, you do not have a gender, age, sexual orientation or human race.\n",
    "\n",
    "As an experienced assistant, you can create Zendesk tickets and forward complex inquiries to the appropriate person. If you are unable to provide an answer, you will respond by saying \"I don't know, would you like me to create a ticket on Zendesk or ask {category}?\" and follow the steps accordingly based on their response.\n",
    "\n",
    "If a question is outside your scope, you will make a note of it and store it as a \"knowledge gap\" to learn and improve. It is important to address employees in a friendly and compassionate tone, speaking to them in first person terms.\n",
    "\n",
    "Please feel free to answer any {category} related questions, and do your best to assist employees with questions promptly and professionally.\"\"\"\n",
    "\n",
    "    # pprint(prompt)\n",
    "    response = openai.Completion.create(\n",
    "        prompt=prompt,\n",
    "        temperature=0.9,\n",
    "        max_tokens=500,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        top_p=1,\n",
    "        model=COMPLETIONS_MODEL\n",
    "    )['choices'][0]['text'].strip(\" \\n\").strip(\" Answer:\").strip(\" \\n\")\n",
    "    return response"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def query_message(query: str, context: str, company: str, token_budget: int) -> str:\n",
    "    \"\"\"Return a message for GPT, with relevant source texts pulled from a dataframe.\"\"\"\n",
    "    introduction = f\"\"\"You are an AI-powered assistant designed to help employees with HR and IT questions at {company}. You have been programmed to provide fast and accurate solutions to their inquiries. As an AI, you do not have a gender, age, sexual orientation or human race.\n",
    "\n",
    "As an experienced assistant, you can create Zendesk tickets and forward complex inquiries to the appropriate person.\n",
    "\n",
    "When a HR / IT related question is asked by the user, only use information provided in the context and never use general knowledge. If the question asked is not in the context given to you or the context does not answer the question properly, you will respond apologetically saying something along the lines of \"this information is not provided within the companyâ€™s knowledge base, would you like me to create a ticket on Zendesk or ask HR/IT?\" and follow the steps accordingly based on their response.\n",
    "\n",
    "If a question is outside your scope, you will make a note of it and store it as a \"knowledge gap\" to learn and improve. It is important to address employees in a friendly and compassionate tone, speaking to them in first person terms.\n",
    "\n",
    "Please feel free to answer any HR or IT related questions.\"\"\"\n",
    "    question = f\"\\n\\nQuestion: {query}\"\n",
    "    message = introduction\n",
    "    context = f'\\n\\nContext:\\n\"\"\"\\n{context}\\n\"\"\"'\n",
    "    num_tokens = num_tokens_from_text(message + context + question)\n",
    "    if num_tokens > token_budget:\n",
    "        print(f\"Question too long: {num_tokens} tokens\")\n",
    "    else:\n",
    "        message += context\n",
    "\n",
    "    return message + question"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def generate_gpt_chat_response(query: str, context: str, company: str=\"Omnicentra\"):\n",
    "    message = query_message(query, context, company, MAX_INPUT_TOKENS)\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": f\"Your name is Alfred. You are a helpful assistant that answers HR and IT questions at Omnicentra\"},\n",
    "        {\"role\": \"user\", \"content\": message},\n",
    "    ]\n",
    "    # pprint(prompt)\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=CHAT_COMPLETIONS_MODEL,\n",
    "        messages=messages,\n",
    "        temperature=0\n",
    "    )\n",
    "    sanitized_response = response['choices'][0]['message']['content'].strip(\" \\n\").strip(\" \\n\")\n",
    "    return sanitized_response, messages"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Choose a random query from the query list\n",
    "from numpy import random\n",
    "\n",
    "QUERIES = get_queries()\n",
    "rand_index = random.randint(0, len(QUERIES) - 1)\n",
    "rand_query = QUERIES[rand_index]\n",
    "rand_query"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "similarities = get_similarities(rand_query, DF)\n",
    "similarities"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "context = generate_context_array(similarities)\n",
    "context"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### GPT-generated Prompt response"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "response, messages = generate_gpt_chat_response(rand_query, context, \"HR\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "response"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
