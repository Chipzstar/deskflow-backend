{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Zendesk articles for search\n",
    "\n",
    "This notebook shows how we prepared a dataset of Wikipedia articles for search, used in [Question_answering_using_embeddings.ipynb](Question_answering_using_embeddings.ipynb).\n",
    "\n",
    "Procedure:\n",
    "\n",
    "0. Prerequisites: Import libraries, set API key (if needed)\n",
    "1. Collect: We download a few hundred Wikipedia articles about the 2022 Olympics\n",
    "2. Chunk: Documents are split into short, semi-self-contained sections to be embedded\n",
    "3. Embed: Each section is embedded with the OpenAI API\n",
    "4. Store: Embeddings are saved in a CSV file (for large datasets, use a vector database)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Prerequisites\n",
    "\n",
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-04T08:41:42.957806800Z",
     "start_time": "2023-05-04T08:41:41.522805400Z"
    }
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import mwclient  # for downloading example Wikipedia articles\n",
    "import mwparserfromhell  # for splitting Wikipedia articles into sections\n",
    "import openai  # for generating embeddings\n",
    "import numpy as np  # for arrays to store embeddings\n",
    "import pandas as pd  # for DataFrames to store article sections and embeddings\n",
    "import re  # for cutting <ref> links out of Wikipedia articles\n",
    "import tiktoken  # for counting tokens\n",
    "from datetime import datetime\n",
    "# Import the Zenpy Class\n",
    "from zenpy import Zenpy\n",
    "from zenpy.lib.api_objects import Ticket\n",
    "from pprint import pprint\n",
    "from scipy import spatial  # for calculating vector similarities for search\n",
    "import typing  # for type hints\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install any missing libraries with `pip install` in your terminal. E.g.,\n",
    "\n",
    "```zsh\n",
    "pip install openai\n",
    "```\n",
    "\n",
    "(You can also do this in a notebook cell with `!pip install openai`.)\n",
    "\n",
    "If you install any libraries, be sure to restart the notebook kernel."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set API key (if needed)\n",
    "\n",
    "Note that the OpenAI library will try to read your API key from the `OPENAI_API_KEY` environment variable. If you haven't already, set this environment variable by following [these instructions](https://help.openai.com/en/articles/5112595-best-practices-for-api-key-safety)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "openai.organization = \"org-C15lzQ0mQYcGkjGrpiBPk2Hb\"\n",
    "openai.api_key = \"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\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-04T08:41:42.972805200Z",
     "start_time": "2023-05-04T08:41:42.960809900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "MAX_INPUT_TOKENS = 8191\n",
    "COMPLETIONS_MODEL = \"text-davinci-003\"\n",
    "CHAT_COMPLETIONS_MODEL=\"gpt-3.5-turbo\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-04T08:41:43.011807Z",
     "start_time": "2023-05-04T08:41:42.973803400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Configure Zendesk API config\n",
    "\n",
    "The Zendesk API is configured in the [Zendesk dashboard](https://app.zendesk.com/hc/en-us/articles/360001111134-Zendesk-API-Configuration)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Zenpy accepts an API token\n",
    "creds = {\n",
    "    \"email\": \"chisom@exam-genius.com\",\n",
    "    \"token\": \"1ASu216KqW6p0BHrBIOSAYaBlax2NmHvRu5rCAAk\",\n",
    "    \"subdomain\": \"omnicentra\",\n",
    "}\n",
    "\n",
    "# Default\n",
    "zenpy_client = Zenpy(**creds)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-04T08:41:43.012813700Z",
     "start_time": "2023-05-04T08:41:42.992809100Z"
    }
   }
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Collect articles\n",
    "\n",
    "In this example, we'll download a few hundred Wikipedia articles related to the 2022 Winter Olympics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-04T08:41:43.019805Z",
     "start_time": "2023-05-04T08:41:43.011807Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_date_string():\n",
    "    return datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "\n",
    "def fetch_zendesk_sections():\n",
    "    sections = []\n",
    "    for section in zenpy_client.help_center.sections():\n",
    "        if section.name == \"IT Queries\":\n",
    "            section.name = \"IT\"\n",
    "        else:\n",
    "            section.name = \"HR\"\n",
    "        sections.append(section)\n",
    "        pass\n",
    "    return sections\n",
    "\n",
    "\n",
    "def fetch_all_zendesk_articles():\n",
    "    articles = zenpy_client.help_center.articles()\n",
    "    for article in articles:\n",
    "        pprint(article)\n",
    "        pass\n",
    "    return articles\n",
    "\n",
    "\n",
    "def fetch_zendesk_articles_by_section(sections):\n",
    "    my_articles = []\n",
    "    for _section in sections:\n",
    "        category = \"IT\" if _section.name == \"IT\" else \"HR\"\n",
    "        print(f\"Searching for articles in section {_section.name}\")\n",
    "        articles = zenpy_client.help_center.sections.articles(section=_section)\n",
    "        print(f\"Found {len(articles)} articles in section {_section}\")\n",
    "        for article in articles:\n",
    "            # pprint(\"--------------------------------------------------------------------------------------------------\")\n",
    "            my_articles.append((article.title, article.body, category))\n",
    "            pass\n",
    "    return my_articles"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Fetch All Article sections"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Section(id=8592685682972), Section(id=8592685671324)]\n"
     ]
    }
   ],
   "source": [
    "article_sections = fetch_zendesk_sections()\n",
    "print(article_sections)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-04T08:41:43.716811900Z",
     "start_time": "2023-05-04T08:41:43.019805Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HR\n",
      "IT\n"
     ]
    }
   ],
   "source": [
    "for section in article_sections:\n",
    "    print(section.name)\n",
    "    pass"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-04T08:41:43.775803900Z",
     "start_time": "2023-05-04T08:41:43.718809600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Fetch all articles for each section"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for articles in section HR\n",
      "Found 15 articles in section Section(id=8592685682972)\n",
      "Searching for articles in section IT\n",
      "Found 23 articles in section Section(id=8592685671324)\n"
     ]
    },
    {
     "data": {
      "text/plain": "38"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles = fetch_zendesk_articles_by_section(article_sections)\n",
    "len(articles)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-04T08:41:44.425522300Z",
     "start_time": "2023-05-04T08:41:43.734806100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def create_txt_knowledge_base(articles, path: str):\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "\n",
    "    with open(f\"{path}/base.txt\", \"w\") as file:\n",
    "        for article in articles:\n",
    "            file.write(article[0] + \"\\n\" + article[1] + \"\\n\" + article[2] + \"\\n\\n\")\n",
    "            pass\n",
    "    return True"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-04T08:41:44.470546500Z",
     "start_time": "2023-05-04T08:41:44.429525Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_txt_knowledge_base(articles, f\"knowledge_base/{get_date_string()}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-04T08:41:44.471551800Z",
     "start_time": "2023-05-04T08:41:44.446548600Z"
    }
   }
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chunk documents\n",
    "\n",
    "Now that we have our reference documents, we need to prepare them for search.\n",
    "\n",
    "Because GPT can only read a limited amount of text at once, we'll split each document into chunks short enough to be read.\n",
    "\n",
    "For this specific example on Wikipedia articles, we'll:\n",
    "- Remove all html syntax tags (e.g., \\<ref>\\, \\<div>\\), whitespace, and super short sections\n",
    "- Clean up the text by removing reference tags (e.g., <ref>), whitespace, and super short sections\n",
    "- Split each article into sections\n",
    "- Prepend titles and subtitles to each section's text, to help GPT understand the context\n",
    "- If a section is long (say, > 1,600 tokens), we'll recursively split it into smaller sections, trying to split along semantic boundaries like paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-04T08:41:44.488545500Z",
     "start_time": "2023-05-04T08:41:44.460548600Z"
    }
   },
   "outputs": [],
   "source": [
    "def num_tokens_from_text(string: str, encoding_name: str = \"cl100k_base\") -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def clean_up_text(articles):\n",
    "    cleaned_articles = []\n",
    "    for title, body, category in articles:\n",
    "        cleaned_body = BeautifulSoup(body, \"html.parser\").get_text()\n",
    "        if num_tokens_from_text(title.strip() + cleaned_body.strip()) > MAX_INPUT_TOKENS:\n",
    "            left = body[:MAX_INPUT_TOKENS]\n",
    "            right = body[MAX_INPUT_TOKENS:]\n",
    "            cleaned_articles.append((title, left, category))\n",
    "            cleaned_articles.append((title, right, category))\n",
    "        else:\n",
    "            cleaned_articles.append((title, cleaned_body, category))\n",
    "    pass\n",
    "    return cleaned_articles\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-04T08:41:44.531558600Z",
     "start_time": "2023-05-04T08:41:44.493548500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "(38, 3)"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CLEANED_ARTICLES = clean_up_text(articles)\n",
    "np.array(CLEANED_ARTICLES).shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-04T08:41:44.893551Z",
     "start_time": "2023-05-04T08:41:44.505551100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-04T08:41:44.992546800Z",
     "start_time": "2023-05-04T08:41:44.897549900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How do I access my payslip and other payroll-related documents?\n"
     ]
    },
    {
     "data": {
      "text/plain": "'Employees can access their payslip and other payroll-related documents throug'"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HR\n",
      "--------------------------------------------------\n",
      "What should I do if I have concerns about harassment or discrimination in the workplace?\n"
     ]
    },
    {
     "data": {
      "text/plain": "'Employees should report any concerns about harassment or discrimination to th'"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HR\n",
      "--------------------------------------------------\n",
      "How do I update my personal information, such as my address or emergency contact?\n"
     ]
    },
    {
     "data": {
      "text/plain": "'Employees can update their personal information through our HR system. They s'"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HR\n",
      "--------------------------------------------------\n",
      "What is the process for submitting an expense report?\n"
     ]
    },
    {
     "data": {
      "text/plain": "'Employees can submit an expense report through our HR system. They should inc'"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HR\n",
      "--------------------------------------------------\n",
      "How can I obtain a copy of my employment contract?\n"
     ]
    },
    {
     "data": {
      "text/plain": "'Employees can obtain a copy of their employment contract by contacting the HR'"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HR\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# print example data\n",
    "for article in CLEANED_ARTICLES[:5]:\n",
    "    print(article[0])\n",
    "    display(article[1][:77])\n",
    "    print(article[2])\n",
    "    print(\"-\"*50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll recursively split long sections into smaller sections.\n",
    "\n",
    "There's no perfect recipe for splitting text into sections.\n",
    "\n",
    "Some tradeoffs include:\n",
    "- Longer sections may be better for questions that require more context\n",
    "- Longer sections may be worse for retrieval, as they may have more topics muddled together\n",
    "- Shorter sections are better for reducing costs (which are proportional to the number of tokens)\n",
    "- Shorter sections allow more sections to be retrieved, which may help with recall\n",
    "- Overlapping sections may help prevent answers from being cut by section boundaries\n",
    "\n",
    "Here, we'll use a simple approach and limit sections to 1,600 tokens each, recursively halving any sections that are too long. To avoid cutting in the middle of useful sentences, we'll split along paragraph boundaries when possible."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Embed document chunks\n",
    "\n",
    "Now that we've split our library into shorter self-contained strings, we can compute embeddings for each.\n",
    "\n",
    "(For large embedding jobs, use a script like [api_request_parallel_processor.py](api_request_parallel_processor.py) to parallelize requests while throttling to stay under rate limits.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-04T08:41:44.993547300Z",
     "start_time": "2023-05-04T08:41:44.924549600Z"
    }
   },
   "outputs": [],
   "source": [
    "# calculate embeddings\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002\"  # OpenAI's best embeddings as of Apr 2023\n",
    "BATCH_SIZE = 1000  # you can submit up to 2048 embedding inputs per request\n",
    "\n",
    "\n",
    "def calculate_embeddings(articles):\n",
    "    titles = []\n",
    "    content = []\n",
    "    categories = []\n",
    "    embeddings = []\n",
    "    for batch_start in range(0, len(articles), BATCH_SIZE):\n",
    "        batch_end = batch_start + BATCH_SIZE\n",
    "        batch = articles[batch_start:batch_end]\n",
    "        titles.extend([article[0] for article in batch])\n",
    "        content.extend([article[1] for article in batch])\n",
    "        categories.extend([article[2] for article in batch])\n",
    "        batch_text = [title + \" \" + body for title, body, category in batch]\n",
    "        print(f\"Batch {batch_start} to {batch_end - 1}\")\n",
    "        response = openai.Embedding.create(model=EMBEDDING_MODEL, input=batch_text)\n",
    "        for i, be in enumerate(response[\"data\"]):\n",
    "            assert i == be[\"index\"]  # double check embeddings are in same order as input\n",
    "        batch_embeddings = [e[\"embedding\"] for e in response[\"data\"]]\n",
    "        embeddings.extend(batch_embeddings)\n",
    "\n",
    "    return pd.DataFrame({\"titles\": titles, \"content\": content, \"categories\": categories, \"embedding\": embeddings}), embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 to 999\n"
     ]
    }
   ],
   "source": [
    "DF, EMBEDDINGS = calculate_embeddings(CLEANED_ARTICLES)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-04T08:41:46.095318500Z",
     "start_time": "2023-05-04T08:41:44.955545200Z"
    }
   }
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Store document chunks and embeddings\n",
    "\n",
    "Because this example only uses a few thousand strings, we'll store them in a CSV file.\n",
    "\n",
    "(For larger datasets, use a vector database, which will be more performant.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-04T08:41:46.109319500Z",
     "start_time": "2023-05-04T08:41:46.095318500Z"
    }
   },
   "outputs": [],
   "source": [
    "# save document chunks and embeddings\n",
    "def save_dataframe_to_csv(df: pd.DataFrame, path: str, filename: str):\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "        print(f\"Created {path}\")\n",
    "    df.to_csv(f\"{path}/{filename}\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "save_dataframe_to_csv(DF, f\"data/{get_date_string()}\", \"zendesk_vector_embeddings.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-04T08:41:46.204317Z",
     "start_time": "2023-05-04T08:41:46.112320300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Store embeddings in Pinecone database"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chiso\\.virtualenvs\\deskflow-backend-slkk3tYw\\lib\\site-packages\\pinecone\\index.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "# Initialise pinecone client with valid API key and environment\n",
    "import pinecone\n",
    "\n",
    "pinecone.init(api_key=\"50f995ae-f134-4a60-8aba-edf67c153790\", environment=\"us-west1-gcp-free\")\n",
    "# Connect to the \"Alfred\" index\n",
    "index = pinecone.Index(\"alfred\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-04T08:41:48.074154600Z",
     "start_time": "2023-05-04T08:41:46.206321500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# Insert the vector embeddings into the index\n",
    "from tqdm.auto import tqdm  # this is our progress bar\n",
    "\n",
    "\n",
    "def store_embeddings_into_pinecone(embeddings: np.ndarray, index: pinecone.Index):\n",
    "    batch_size = 32  # process everything in batches of 32\n",
    "    for i in tqdm(range(0, len(DF), batch_size)):\n",
    "        i_end = min(i + batch_size, len(DF))\n",
    "        batch = DF[i: i + batch_size]\n",
    "        embeddings_batch = batch[\"embedding\"]\n",
    "        ids_batch = [str(n) for n in range(i, i_end)]\n",
    "        # prep metadata and upsert batch\n",
    "        meta = [{'title': titles, \"content\": content, \"category\": categories} for titles, content, categories, embeddings in batch.to_numpy()]\n",
    "        to_upsert = zip(ids_batch, embeddings_batch, meta)\n",
    "        print(to_upsert)\n",
    "        index.upsert(vectors=list(to_upsert))\n",
    "        # upsert to Pinecone"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-04T08:41:48.094152700Z",
     "start_time": "2023-05-04T08:41:48.075155Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ffe06bd2160d4f6095b255c8f94f0af9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<zip object at 0x000001F04E9D1E00>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chiso\\.virtualenvs\\deskflow-backend-slkk3tYw\\lib\\site-packages\\pinecone\\core\\client\\rest.py:45: DeprecationWarning: HTTPResponse.getheader() is deprecated and will be removed in urllib3 v2.1.0. Instead use HTTPResponse.headers.get(name, default).\n",
      "  return self.urllib3_response.getheader(name, default)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<zip object at 0x000001F04E9DEE00>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chiso\\.virtualenvs\\deskflow-backend-slkk3tYw\\lib\\site-packages\\pinecone\\core\\client\\rest.py:45: DeprecationWarning: HTTPResponse.getheader() is deprecated and will be removed in urllib3 v2.1.0. Instead use HTTPResponse.headers.get(name, default).\n",
      "  return self.urllib3_response.getheader(name, default)\n"
     ]
    }
   ],
   "source": [
    "store_embeddings_into_pinecone(EMBEDDINGS, index)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-04T08:41:50.522402800Z",
     "start_time": "2023-05-04T08:41:48.095155600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6. Create VE for test query and retrieve embeddings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "QUERIES = []\n",
    "# removing the new line characters\n",
    "with open('test/queries.txt') as f:\n",
    "    lines = [line.rstrip() for line in f]\n",
    "    for line in lines:\n",
    "        QUERIES.append(line)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-04T08:41:50.530401300Z",
     "start_time": "2023-05-04T08:41:50.514404Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "# search function\n",
    "def strings_ranked_by_relatedness(\n",
    "        query: str,\n",
    "        df: pd.DataFrame,\n",
    "        relatedness_fn=lambda x, y: 1 - spatial.distance.cosine(x, y),\n",
    "        top_n: int = 100\n",
    ") -> tuple[list[str], list[float]]:\n",
    "    \"\"\"Returns a list of strings and relatednesses, sorted from most related to least.\"\"\"\n",
    "    query_embedding_response = openai.Embedding.create(\n",
    "        model=EMBEDDING_MODEL,\n",
    "        input=query,\n",
    "    )\n",
    "    query_embedding = query_embedding_response[\"data\"][0][\"embedding\"]\n",
    "    strings_and_relatednesses = [\n",
    "        (row[\"content\"], relatedness_fn(query_embedding, row[\"embedding\"]))\n",
    "        for i, row in df.iterrows()\n",
    "    ]\n",
    "    strings_and_relatednesses.sort(key=lambda x: x[1], reverse=True)\n",
    "    strings, relatednesses = zip(*strings_and_relatednesses)\n",
    "    return strings[:top_n], relatednesses[:top_n], query_embedding"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-04T08:41:50.588409900Z",
     "start_time": "2023-05-04T08:41:50.535404300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/82 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bf7f900b78164bb689527094fffc0f64"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "SCORES = []\n",
    "ANSWERS = []\n",
    "EMBEDDINGS = []\n",
    "for QUERY in tqdm(QUERIES):\n",
    "    strings, relatednesses, embedding = strings_ranked_by_relatedness(QUERY, DF, top_n=1)\n",
    "    for string, relatedness in zip(strings, relatednesses):\n",
    "        ANSWERS.append(string)\n",
    "        SCORES.append(\"%.3f\" % relatedness)\n",
    "        EMBEDDINGS.append(embedding)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-04T08:42:26.739204800Z",
     "start_time": "2023-05-04T08:41:50.549405900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "results = pd.DataFrame({\"question\": QUERIES, \"top_answer\": ANSWERS, \"match_score\": SCORES, \"embeddings\": EMBEDDINGS})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-04T08:42:26.753246800Z",
     "start_time": "2023-05-04T08:42:26.741210Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "                                            question   \n0  What is the procedure for resigning from Omnic...  \\\n1  How long does it take for an employee to recei...   \n2  What is the procedure for filing a complaint a...   \n3  Can employees participate in external training...   \n4   What is the policy for employee travel expenses?   \n\n                                          top_answer match_score   \n0  The HR policies at Omnicentra include:\\n\\nAnnu...       0.835  \\\n1  Employees can access their payslip and other p...       0.804   \n2  Employees should report any workplace concerns...       0.864   \n3  Employees can access technical training and su...       0.824   \n4  Employees can submit an expense report through...       0.841   \n\n                                          embeddings  \n0  [0.015012592077255249, 0.0005651644896715879, ...  \n1  [-0.008450762368738651, -0.004312270320951939,...  \n2  [0.010676022619009018, -0.005092814099043608, ...  \n3  [-0.025632403790950775, -0.043561942875385284,...  \n4  [0.015678646042943, -0.003156952327117324, 0.0...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>question</th>\n      <th>top_answer</th>\n      <th>match_score</th>\n      <th>embeddings</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>What is the procedure for resigning from Omnic...</td>\n      <td>The HR policies at Omnicentra include:\\n\\nAnnu...</td>\n      <td>0.835</td>\n      <td>[0.015012592077255249, 0.0005651644896715879, ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>How long does it take for an employee to recei...</td>\n      <td>Employees can access their payslip and other p...</td>\n      <td>0.804</td>\n      <td>[-0.008450762368738651, -0.004312270320951939,...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>What is the procedure for filing a complaint a...</td>\n      <td>Employees should report any workplace concerns...</td>\n      <td>0.864</td>\n      <td>[0.010676022619009018, -0.005092814099043608, ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Can employees participate in external training...</td>\n      <td>Employees can access technical training and su...</td>\n      <td>0.824</td>\n      <td>[-0.025632403790950775, -0.043561942875385284,...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>What is the policy for employee travel expenses?</td>\n      <td>Employees can submit an expense report through...</td>\n      <td>0.841</td>\n      <td>[0.015678646042943, -0.003156952327117324, 0.0...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-04T08:42:26.784248200Z",
     "start_time": "2023-05-04T08:42:26.755249700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "save_dataframe_to_csv(results, f\"data/{get_date_string()}/\", \"zendesk_query_embeddings.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-04T08:42:27.082245500Z",
     "start_time": "2023-05-04T08:42:26.785255200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 7. Use GPT3 model to generate user-friendly answers to the query"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "def generate_gpt_opt_response(record: pd.Series, category: Literal[\"IT\", \"HR\"], company: str=\"Omnicentra\", description: str=\"an AI software company\"):\n",
    "    question = record.question\n",
    "    context = record.top_answer\n",
    "    prompt = f\"\"\"Name: Alfred\n",
    "\n",
    "\"Answer the following question by rephrasing the context below\"\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "You are an AI-powered assistant designed to help employees with {category} questions at {company}. You have been programmed to provide fast and accurate solutions to their inquiries. As an AI, you do not have a gender, age, sexual orientation or human race.\n",
    "\n",
    "As an experienced assistant, you can create Zendesk tickets and forward complex inquiries to the appropriate person. If you are unable to provide an answer, you will respond by saying \"I don't know, would you like me to create a ticket on Zendesk or ask {category}?\" and follow the steps accordingly based on their response.\n",
    "\n",
    "If a question is outside your scope, you will make a note of it and store it as a \"knowledge gap\" to learn and improve. It is important to address employees in a friendly and compassionate tone, speaking to them in first person terms.\n",
    "\n",
    "Please feel free to answer any {category} related questions, and do your best to assist employees with questions promptly and professionally.\"\"\"\n",
    "\n",
    "    # pprint(prompt)\n",
    "    response = openai.Completion.create(\n",
    "        prompt=prompt,\n",
    "        temperature=0.9,\n",
    "        max_tokens=500,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        top_p=1,\n",
    "        model=COMPLETIONS_MODEL\n",
    "    )['choices'][0]['text'].strip(\" \\n\").strip(\" Answer:\").strip(\" \\n\")\n",
    "    return response"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-04T08:42:27.082245500Z",
     "start_time": "2023-05-04T08:42:27.034246800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "def query_message(\n",
    "    query: str,\n",
    "    category: str,\n",
    "    company: str,\n",
    "    content: str,\n",
    "    token_budget: int\n",
    ") -> str:\n",
    "    \"\"\"Return a message for GPT, with relevant source texts pulled from a dataframe.\"\"\"\n",
    "    introduction = f\"\"\"You are an AI-powered assistant designed to help employees with {category} questions at {company}. You have been programmed to provide fast and accurate solutions to their inquiries. As an AI, you do not have a gender, age, sexual orientation or human race.\n",
    "\n",
    "As an experienced assistant, you can create Zendesk tickets and forward complex inquiries to the appropriate person. If you are unable to provide an answer, you will respond by saying \"I don't know, would you like me to create a ticket on Zendesk or ask {category}?\" and follow the steps accordingly based on their response.\n",
    "\n",
    "If a question is outside your scope, you will make a note of it and store it as a \"knowledge gap\" to learn and improve. It is important to address employees in a friendly and compassionate tone, speaking to them in first person terms.\n",
    "\n",
    "Please feel free to answer any {category} related questions, and do your best to assist employees with questions promptly and professionally.\"\"\"\n",
    "    question = f\"\\n\\nQuestion: {query}\"\n",
    "    message = introduction\n",
    "    context = f'\\n\\nContext:\\n\"\"\"\\n{content}\\n\"\"\"'\n",
    "    num_tokens = num_tokens_from_text(message + context + question)\n",
    "    if num_tokens > token_budget:\n",
    "        print(f\"Question too long: {num_tokens} tokens\")\n",
    "    else:\n",
    "        message += context\n",
    "\n",
    "    return message + question"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-04T08:42:27.083249900Z",
     "start_time": "2023-05-04T08:42:27.040244Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "def generate_gpt_chat_response(record: pd.Series, category: Literal[\"IT\", \"HR\"], company: str=\"Omnicentra\", description: str=\"an AI software company\"):\n",
    "    message = query_message(record.question, category, company,  record.top_answer, MAX_INPUT_TOKENS)\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": f\"You answer {category} questions at {company}\"},\n",
    "        {\"role\": \"user\", \"content\": message},\n",
    "    ]\n",
    "    # pprint(prompt)\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=CHAT_COMPLETIONS_MODEL,\n",
    "        messages=messages,\n",
    "        temperature=0\n",
    "    )['choices'][0]['message']['content'].strip(\" \\n\").strip(\" Answer:\").strip(\" \\n\")\n",
    "    return response"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-04T08:42:27.084244900Z",
     "start_time": "2023-05-04T08:42:27.055249400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Can employees receive time off in lieu of overtime pay at Omnicentra?\n",
      "Answer: The HR policies at Omnicentra include:\n",
      "\n",
      "Annual Leave: Employees are entitled to 25 days of annual leave per year, which must be taken in consultation with their line manager.\n",
      "Sick Leave: If an employee is unable to come to work due to illness, they should notify their line manager as soon as possible and provide a doctor's note if they are absent for more than three days.\n",
      "Working from Home: Employees may work from home if they have a good reason and have obtained approval from their line manager.\n",
      "Parental Leave: Employees are entitled to parental leave if they have worked at Omnicentra for at least one year.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Choose a random query from the query list\n",
    "from numpy import random\n",
    "rand_index = random.randint(0, len(results) - 1)\n",
    "record = results[['question', 'top_answer']].iloc[rand_index]\n",
    "print(f\"Question: {record.question}\")\n",
    "print(f\"Answer: {record.top_answer}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-04T08:42:27.102243200Z",
     "start_time": "2023-05-04T08:42:27.074250700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### GPT-generated Prompt response"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "'Omnicentra does not offer time off in lieu of overtime pay.'"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_gpt_opt_response(record, \"HR\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-04T08:42:29.221393800Z",
     "start_time": "2023-05-04T08:42:27.089252200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "'Yes, employees at Omnicentra can receive time off in lieu of overtime pay. This means that if an employee works overtime, they can choose to take time off instead of receiving extra pay. However, this must be agreed upon by the employee and their line manager. It is important to note that time off in lieu must be taken within a reasonable timeframe and cannot be accumulated indefinitely.'"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_gpt_chat_response(record, \"HR\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-04T08:42:35.066501900Z",
     "start_time": "2023-05-04T08:42:29.223399600Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
