{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Zendesk articles for search\n",
    "\n",
    "This notebook shows how we prepared a dataset of Wikipedia articles for search, used in [Question_answering_using_embeddings.ipynb](Question_answering_using_embeddings.ipynb).\n",
    "\n",
    "Procedure:\n",
    "\n",
    "0. Prerequisites: Import libraries, set API key (if needed)\n",
    "1. Collect: We download a few hundred Wikipedia articles about the 2022 Olympics\n",
    "2. Chunk: Documents are split into short, semi-self-contained sections to be embedded\n",
    "3. Embed: Each section is embedded with the OpenAI API\n",
    "4. Store: Embeddings are saved in a CSV file (for large datasets, use a vector database)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Prerequisites\n",
    "\n",
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import mwclient  # for downloading example Wikipedia articles\n",
    "import mwparserfromhell  # for splitting Wikipedia articles into sections\n",
    "import openai  # for generating embeddings\n",
    "import numpy as np  # for arrays to store embeddings\n",
    "import pandas as pd  # for DataFrames to store article sections and embeddings\n",
    "import re  # for cutting <ref> links out of Wikipedia articles\n",
    "import tiktoken  # for counting tokens\n",
    "from datetime import datetime\n",
    "# Import the Zenpy Class\n",
    "from zenpy import Zenpy\n",
    "from zenpy.lib.api_objects import Ticket\n",
    "from pprint import pprint\n",
    "from scipy import spatial  # for calculating vector similarities for search\n",
    "import typing  # for type hints\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install any missing libraries with `pip install` in your terminal. E.g.,\n",
    "\n",
    "```zsh\n",
    "pip install openai\n",
    "```\n",
    "\n",
    "(You can also do this in a notebook cell with `!pip install openai`.)\n",
    "\n",
    "If you install any libraries, be sure to restart the notebook kernel."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set API key (if needed)\n",
    "\n",
    "Note that the OpenAI library will try to read your API key from the `OPENAI_API_KEY` environment variable. If you haven't already, set this environment variable by following [these instructions](https://help.openai.com/en/articles/5112595-best-practices-for-api-key-safety)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "openai.organization = os.environ[\"OPENAI_ORG_ID\"]\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "MAX_INPUT_TOKENS = 8191\n",
    "COMPLETIONS_MODEL = \"text-davinci-003\"\n",
    "CHAT_COMPLETIONS_MODEL=\"gpt-3.5-turbo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Configure Zendesk API config\n",
    "\n",
    "The Zendesk API is configured in the [Zendesk dashboard](https://app.zendesk.com/hc/en-us/articles/360001111134-Zendesk-API-Configuration)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ZENDESK_API_KEY = os.environ[\"ZENDESK_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Zenpy accepts an API token\n",
    "creds = {\n",
    "    \"email\": \"chisom@exam-genius.com\",\n",
    "    \"token\": ZENDESK_API_KEY,\n",
    "    \"subdomain\": \"omnicentra\",\n",
    "}\n",
    "\n",
    "# Default\n",
    "zenpy_client = Zenpy(**creds)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Collect articles\n",
    "\n",
    "Here we define all functions for collecting articles stored within the Zendesk Help Centre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date_string():\n",
    "    return datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "\n",
    "def fetch_zendesk_sections():\n",
    "    sections = []\n",
    "    for section in zenpy_client.help_center.sections():\n",
    "        if section.name == \"IT Queries\":\n",
    "            section.name = \"IT\"\n",
    "        else:\n",
    "            section.name = \"HR\"\n",
    "        sections.append(section)\n",
    "        pass\n",
    "    return sections\n",
    "\n",
    "\n",
    "def fetch_all_zendesk_articles():\n",
    "    articles = zenpy_client.help_center.articles()\n",
    "    for article in articles:\n",
    "        pprint(article)\n",
    "        pass\n",
    "    return articles\n",
    "\n",
    "\n",
    "def fetch_zendesk_articles_by_section(sections):\n",
    "    my_articles = []\n",
    "    for _section in sections:\n",
    "        category = \"IT\" if _section.name == \"IT\" else \"HR\"\n",
    "        print(f\"Searching for articles in section {_section.name}\")\n",
    "        articles = zenpy_client.help_center.sections.articles(section=_section)\n",
    "        print(f\"Found {len(articles)} articles in section {_section}\")\n",
    "        for article in articles:\n",
    "            # pprint(\"--------------------------------------------------------------------------------------------------\")\n",
    "            my_articles.append((article.title, article.body, category))\n",
    "            pass\n",
    "    return my_articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Fetch All Article sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Section(id=8592685682972), Section(id=8592685671324)]\n"
     ]
    }
   ],
   "source": [
    "article_sections = fetch_zendesk_sections()\n",
    "print(article_sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HR\n",
      "IT\n"
     ]
    }
   ],
   "source": [
    "for section in article_sections:\n",
    "    print(section.name)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Fetch all articles for each section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for articles in section HR\n",
      "Found 15 articles in section Section(id=8592685682972)\n",
      "Searching for articles in section IT\n",
      "Found 23 articles in section Section(id=8592685671324)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles = fetch_zendesk_articles_by_section(article_sections)\n",
    "len(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def create_txt_knowledge_base(articles, path: str):\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "\n",
    "    with open(f\"{path}/base.txt\", \"w\") as file:\n",
    "        for article in articles:\n",
    "            file.write(article[0] + \"\\n\" + article[1] + \"\\n\" + article[2] + \"\\n\\n\")\n",
    "            pass\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_txt_knowledge_base(articles, f\"knowledge_base/{get_date_string()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chunk documents\n",
    "\n",
    "Now that we have our reference documents, we need to prepare them for search.\n",
    "\n",
    "Because GPT can only read a limited amount of text at once, we'll split each document into chunks short enough to be read.\n",
    "\n",
    "For this specific example on Zendesk articles, we'll:\n",
    "- Remove all html syntax tags (e.g., \\<ref>\\, \\<div>\\), whitespace, and super short sections\n",
    "- Clean up the text by removing reference tags (e.g., <ref>), whitespace, and super short sections\n",
    "- Split each article into sections\n",
    "- Prepend titles and subtitles to each section's text, to help GPT understand the context\n",
    "- If a section is long (say, > 1,600 tokens), we'll recursively split it into smaller sections, trying to split along semantic boundaries like paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens_from_text(string: str, encoding_name: str = \"cl100k_base\") -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def clean_up_text(articles):\n",
    "    cleaned_articles = []\n",
    "    for title, body, category in articles:\n",
    "        cleaned_body = BeautifulSoup(body, \"html.parser\").get_text()\n",
    "        if num_tokens_from_text(title.strip() + cleaned_body.strip()) > MAX_INPUT_TOKENS:\n",
    "            left = body[:MAX_INPUT_TOKENS]\n",
    "            right = body[MAX_INPUT_TOKENS:]\n",
    "            cleaned_articles.append((title, left, category))\n",
    "            cleaned_articles.append((title, right, category))\n",
    "        else:\n",
    "            cleaned_articles.append((title, cleaned_body, category))\n",
    "    pass\n",
    "    return cleaned_articles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38, 3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CLEANED_ARTICLES = clean_up_text(articles)\n",
    "np.array(CLEANED_ARTICLES).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How do I access my payslip and other payroll-related documents?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Employees can access their payslip and other payroll-related documents throug'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HR\n",
      "--------------------------------------------------\n",
      "What should I do if I have concerns about harassment or discrimination in the workplace?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Employees should report any concerns about harassment or discrimination to th'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HR\n",
      "--------------------------------------------------\n",
      "How do I update my personal information, such as my address or emergency contact?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Employees can update their personal information through our HR system. They s'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HR\n",
      "--------------------------------------------------\n",
      "What email service does Omnicentra use?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Omnicentra uses Gmail for email. Employees should ensure that their email acc'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IT\n",
      "--------------------------------------------------\n",
      "What communication tool does Omnicentra use for internal communication?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Omnicentra uses Slack for internal communication. Employees should ensure tha'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IT\n",
      "--------------------------------------------------\n",
      "What data analysis tool does Omnicentra use?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Omnicentra uses Looker for data analysis. Employees can access Looker by logg'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IT\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# print example data\n",
    "for article in CLEANED_ARTICLES[:3]:\n",
    "    print(article[0])\n",
    "    display(article[1][:77])\n",
    "    print(article[2])\n",
    "    print(\"-\"*50)\n",
    "\n",
    "for article in reversed(CLEANED_ARTICLES[-3:]):\n",
    "    print(article[0])\n",
    "    display(article[1][:77])\n",
    "    print(article[2])\n",
    "    print(\"-\"*50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll recursively split long sections into smaller sections.\n",
    "\n",
    "There's no perfect recipe for splitting text into sections.\n",
    "\n",
    "Some tradeoffs include:\n",
    "- Longer sections may be better for questions that require more context\n",
    "- Longer sections may be worse for retrieval, as they may have more topics muddled together\n",
    "- Shorter sections are better for reducing costs (which are proportional to the number of tokens)\n",
    "- Shorter sections allow more sections to be retrieved, which may help with recall\n",
    "- Overlapping sections may help prevent answers from being cut by section boundaries\n",
    "\n",
    "Here, we'll use a simple approach and limit sections to 1,600 tokens each, recursively halving any sections that are too long. To avoid cutting in the middle of useful sentences, we'll split along paragraph boundaries when possible."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Embed document chunks\n",
    "\n",
    "Now that we've split our library into shorter self-contained strings, we can compute embeddings for each.\n",
    "\n",
    "(For large embedding jobs, use a script like [api_request_parallel_processor.py](api_request_parallel_processor.py) to parallelize requests while throttling to stay under rate limits.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate embeddings\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002\"  # OpenAI's best embeddings as of Apr 2023\n",
    "BATCH_SIZE = 1000  # you can submit up to 2048 embedding inputs per request\n",
    "\n",
    "\n",
    "def calculate_embeddings(articles):\n",
    "    titles = []\n",
    "    content = []\n",
    "    categories = []\n",
    "    embeddings = []\n",
    "    for batch_start in range(0, len(articles), BATCH_SIZE):\n",
    "        batch_end = batch_start + BATCH_SIZE\n",
    "        batch = articles[batch_start:batch_end]\n",
    "        titles.extend([article[0] for article in batch])\n",
    "        content.extend([article[1] for article in batch])\n",
    "        categories.extend([article[2] for article in batch])\n",
    "        batch_text = [title + \" \" + body for title, body, category in batch]\n",
    "        print(f\"Batch {batch_start} to {batch_end - 1}\")\n",
    "        response = openai.Embedding.create(model=EMBEDDING_MODEL, input=batch_text)\n",
    "        for i, be in enumerate(response[\"data\"]):\n",
    "            assert i == be[\"index\"]  # double check embeddings are in same order as input\n",
    "        batch_embeddings = [e[\"embedding\"] for e in response[\"data\"]]\n",
    "        embeddings.extend(batch_embeddings)\n",
    "\n",
    "    return pd.DataFrame({\"titles\": titles, \"content\": content, \"categories\": categories, \"embedding\": embeddings}), embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 to 999\n"
     ]
    }
   ],
   "source": [
    "DF, EMBEDDINGS = calculate_embeddings(CLEANED_ARTICLES)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Store document chunks and embeddings\n",
    "\n",
    "Because this example only uses a few thousand strings, we'll store them in a CSV file.\n",
    "\n",
    "(For larger datasets, use a vector database, which will be more performant.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save document chunks and embeddings\n",
    "def save_dataframe_to_csv(df: pd.DataFrame, path: str, filename: str):\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "        print(f\"Created {path}\")\n",
    "    df.to_csv(f\"{path}/{filename}\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "save_dataframe_to_csv(DF, f\"data/{get_date_string()}\", \"zendesk_vector_embeddings.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Store embeddings in Pinecone database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "PINECONE_API_KEY=os.environ[\"PINECONE_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chiso\\.virtualenvs\\deskflow-backend-f8YEd6o4\\lib\\site-packages\\pinecone\\index.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "# Initialise pinecone client with valid API key and environment\n",
    "import pinecone\n",
    "\n",
    "pinecone.init(api_key=PINECONE_API_KEY, environment=\"us-west1-gcp-free\")\n",
    "# Connect to the \"Alfred\" index\n",
    "index = pinecone.Index(\"alfred\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Insert the vector embeddings into the index\n",
    "from tqdm.auto import tqdm  # this is our progress bar\n",
    "\n",
    "\n",
    "def store_embeddings_into_pinecone(embeddings: np.ndarray, index: pinecone.Index, email: str = \"chipzstar.dev@googlemail.com\"):\n",
    "    batch_size = 32  # process everything in batches of 32\n",
    "    for i in tqdm(range(0, len(DF), batch_size)):\n",
    "        i_end = min(i + batch_size, len(DF))\n",
    "        batch = DF[i: i + batch_size]\n",
    "        embeddings_batch = batch[\"embedding\"]\n",
    "        ids_batch = [str(n) for n in range(i, i_end)]\n",
    "        # prep metadata and upsert batch\n",
    "        meta = [{'title': titles, \"content\": content, \"category\": categories} for titles, content, categories, embeddings in batch.to_numpy()]\n",
    "        to_upsert = zip(ids_batch, embeddings_batch, meta)\n",
    "        # upsert to Pinecone\n",
    "        index.upsert(vectors=list(to_upsert), namespace=email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "store_embeddings_into_pinecone(EMBEDDINGS, index)\n",
    "DF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Get VEs from Pinecone DB and convert to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector_embeddings_from_pinecone(index_name: str, namespace: str = \"chipzstar.dev@googlemail.com\"):\n",
    "    titles = []\n",
    "    content = []\n",
    "    categories = []\n",
    "    embeddings = []\n",
    "    # Connect to the index <INDEX_NAME> provided\n",
    "    index = pinecone.Index(index_name)\n",
    "    # describe the pincone index\n",
    "    index_stats = index.describe_index_stats()\n",
    "    # extract the total_vector_count\n",
    "    num_vectors = int(index_stats[\"total_vector_count\"])\n",
    "    # Use vector count to fetch all vectors in the index\n",
    "    ids = [str(x) for x in range(0,38)]\n",
    "    vectors = (index.fetch(ids=ids, namespace=namespace))[\"vectors\"]\n",
    "    # Keys = list(vectors.keys())\n",
    "    # Keys.sort(key=int)\n",
    "    # iterate over each vector space and append to pandas dataframe\n",
    "    for i, (k, v) in enumerate(sorted(vectors.items(), key=lambda item: int(item[0]))):\n",
    "        titles.append(v[\"metadata\"][\"title\"])\n",
    "        content.append(v[\"metadata\"][\"content\"])\n",
    "        categories.append(v[\"metadata\"][\"category\"])\n",
    "        embeddings.append(v[\"values\"])\n",
    "    return pd.DataFrame({\"titles\": titles, \"content\": content, \"categories\": categories, \"embedding\": embeddings})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chiso\\.virtualenvs\\deskflow-backend-f8YEd6o4\\lib\\site-packages\\pinecone\\core\\client\\rest.py:45: DeprecationWarning: HTTPResponse.getheader() is deprecated and will be removed in urllib3 v2.1.0. Instead use HTTPResponse.headers.get(name, default).\n",
      "  return self.urllib3_response.getheader(name, default)\n",
      "C:\\Users\\chiso\\.virtualenvs\\deskflow-backend-f8YEd6o4\\lib\\site-packages\\pinecone\\core\\client\\rest.py:45: DeprecationWarning: HTTPResponse.getheader() is deprecated and will be removed in urllib3 v2.1.0. Instead use HTTPResponse.headers.get(name, default).\n",
      "  return self.urllib3_response.getheader(name, default)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "titles        38\n",
       "content       38\n",
       "categories    38\n",
       "embedding     38\n",
       "dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF = get_vector_embeddings_from_pinecone(\"alfred\")\n",
    "DF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# 6. Create VE for test query and retrieve embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def get_queries():\n",
    "    queries = []\n",
    "    # removing the new line characters\n",
    "    with open('test/queries.txt') as f:\n",
    "        lines = [line.rstrip() for line in f]\n",
    "        for line in lines:\n",
    "            queries.append(line)\n",
    "    return queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from openai.embeddings_utils import cosine_similarity, get_embedding\n",
    "\n",
    "# search function\n",
    "def strings_ranked_by_relatedness(\n",
    "        query: str,\n",
    "        df: pd.DataFrame,\n",
    "        relatedness_fn=lambda x, y: cosine_similarity(x, y),\n",
    "        top_n: int = 100\n",
    ") -> tuple[list[str], list[float]]:\n",
    "    \"\"\"Returns a list of strings and relatednesses, sorted from most related to least.\"\"\"\n",
    "    question_vector = get_embedding(query, EMBEDDING_MODEL)\n",
    "    strings_and_relatednesses = [\n",
    "        (row[\"content\"], relatedness_fn(row[\"embedding\"], question_vector), row[\"embedding\"])\n",
    "        for i, row in df.iterrows()\n",
    "    ]\n",
    "    strings_and_relatednesses.sort(key=lambda x: x[1], reverse=True)\n",
    "    strings, relatednesses, embedding = zip(*strings_and_relatednesses)\n",
    "    return strings[:top_n], relatednesses[:top_n], embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def get_similarities(query: typing.List[str], df: pd.DataFrame) -> pd.DataFrame:\n",
    "    SCORES = []\n",
    "    ANSWERS = []\n",
    "    EMBEDDINGS = []\n",
    "    strings, relatednesses, embeddings = strings_ranked_by_relatedness(query, df, top_n=3)\n",
    "    for string, relatedness, embedding in zip(strings, relatednesses, embeddings):\n",
    "        ANSWERS.append(string)\n",
    "        SCORES.append(\"%.3f\" % relatedness)\n",
    "        EMBEDDINGS.append(embedding)\n",
    "\n",
    "    results = pd.DataFrame({\"answers\": ANSWERS, \"match_scores\": SCORES, \"embeddings\": EMBEDDINGS})\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# 7. Combine all top n answers into one chunk of text to use as knowledge base context for GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def generate_context_array(results: pd.DataFrame) -> str:\n",
    "    context_array = []\n",
    "    for i, row in results.iterrows():\n",
    "        context_array.append(row.answers)\n",
    "\n",
    "    context = \"\\n\".join(context_array)\n",
    "    return context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# 8. Use GPT3 model to generate user-friendly answers to the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "def generate_gpt_opt_response(record: pd.Series, category: Literal[\"IT\", \"HR\"], company: str=\"Omnicentra\", description: str=\"an AI software company\"):\n",
    "    question = record.question\n",
    "    context = record.top_answer\n",
    "    prompt = f\"\"\"Name: Alfred\n",
    "\n",
    "\"Answer the following question by rephrasing the context below\"\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "You are an AI-powered assistant designed to help employees with {category} questions at {company}. You have been programmed to provide fast and accurate solutions to their inquiries. As an AI, you do not have a gender, age, sexual orientation or human race.\n",
    "\n",
    "As an experienced assistant, you can create Zendesk tickets and forward complex inquiries to the appropriate person. If you are unable to provide an answer, you will respond by saying \"I don't know, would you like me to create a ticket on Zendesk or ask {category}?\" and follow the steps accordingly based on their response.\n",
    "\n",
    "If a question is outside your scope, you will make a note of it and store it as a \"knowledge gap\" to learn and improve. It is important to address employees in a friendly and compassionate tone, speaking to them in first person terms.\n",
    "\n",
    "Please feel free to answer any {category} related questions, and do your best to assist employees with questions promptly and professionally.\"\"\"\n",
    "\n",
    "    # pprint(prompt)\n",
    "    response = openai.Completion.create(\n",
    "        prompt=prompt,\n",
    "        temperature=1,\n",
    "        max_tokens=500,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        top_p=1,\n",
    "        model=COMPLETIONS_MODEL\n",
    "    )['choices'][0]['text'].strip(\" \\n\").strip(\" Answer:\").strip(\" \\n\")\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def query_message(query: str, context: str, company: str, token_budget: int) -> str:\n",
    "    \"\"\"Return a message for GPT, with relevant source texts pulled from a dataframe.\"\"\"\n",
    "    introduction = f\"\"\"You are an AI-powered assistant designed to help employees with HR and IT questions at {company}. You have been programmed to provide fast and accurate solutions to their inquiries. As an AI, you do not have a gender, age, sexual orientation or human race.\n",
    "\n",
    "As an experienced assistant, you can create Zendesk tickets and forward complex inquiries to the appropriate person.\n",
    "\n",
    "When a HR / IT related question is asked by the user, only use information provided in the context and never use general knowledge. If the question asked is not in the context given to you or the context does not answer the question properly, you will respond apologetically saying something along the lines of \"this information is not provided within the company’s knowledge base, would you like me to create a ticket on Zendesk or ask HR/IT?\" and follow the steps accordingly based on their response.\n",
    "\n",
    "If a question is outside your scope, you will make a note of it and store it as a \"knowledge gap\" to learn and improve. It is important to address employees in a friendly and compassionate tone, speaking to them in first person terms.\n",
    "\n",
    "Please feel free to answer any HR or IT related questions.\"\"\"\n",
    "    question = f\"\\n\\nQuestion: {query}\"\n",
    "    message = introduction\n",
    "    context = f'\\n\\nContext:\\n\"\"\"\\n{context}\\n\"\"\"'\n",
    "    num_tokens = num_tokens_from_text(message + context + question)\n",
    "    if num_tokens > token_budget:\n",
    "        print(f\"Question too long: {num_tokens} tokens\")\n",
    "    else:\n",
    "        message += context\n",
    "\n",
    "    return message + question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def generate_gpt_chat_response(query: str, context: str, company: str=\"Omnicentra\"):\n",
    "    message = query_message(query, context, company, MAX_INPUT_TOKENS)\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": f\"Your name is Alfred. You are a helpful assistant that answers HR and IT questions at Omnicentra\"},\n",
    "        {\"role\": \"user\", \"content\": message},\n",
    "    ]\n",
    "    # pprint(prompt)\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=CHAT_COMPLETIONS_MODEL,\n",
    "        messages=messages,\n",
    "        temperature=0\n",
    "    )\n",
    "    sanitized_response = response['choices'][0]['message']['content'].strip(\" \\n\").strip(\" \\n\")\n",
    "    return sanitized_response, messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Is there a probationary period for new employees at Omnicentra?'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Choose a random query from the query list\n",
    "from numpy import random\n",
    "\n",
    "QUERIES = get_queries()\n",
    "rand_index = random.randint(0, len(QUERIES) - 1)\n",
    "rand_query = QUERIES[rand_index]\n",
    "rand_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answers</th>\n",
       "      <th>match_scores</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The HR policies at Omnicentra include:\\n\\nAnnu...</td>\n",
       "      <td>0.855</td>\n",
       "      <td>[0.0110743828, 0.0120583316, 0.00238558184, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Omnicentra provides employees with laptops and...</td>\n",
       "      <td>0.841</td>\n",
       "      <td>[0.0182543714, -0.00493826624, 0.0105516203, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Omnicentra offers the following benefits:\\n \\n...</td>\n",
       "      <td>0.836</td>\n",
       "      <td>[0.000611283875, 0.005420662, 0.0213357266, -0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             answers match_scores  \\\n",
       "0  The HR policies at Omnicentra include:\\n\\nAnnu...        0.855   \n",
       "1  Omnicentra provides employees with laptops and...        0.841   \n",
       "2  Omnicentra offers the following benefits:\\n \\n...        0.836   \n",
       "\n",
       "                                          embeddings  \n",
       "0  [0.0110743828, 0.0120583316, 0.00238558184, -0...  \n",
       "1  [0.0182543714, -0.00493826624, 0.0105516203, -...  \n",
       "2  [0.000611283875, 0.005420662, 0.0213357266, -0...  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarities = get_similarities(rand_query, DF)\n",
    "similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The HR policies at Omnicentra include:\\n\\nAnnual Leave: Employees are entitled to 25 days of annual leave per year, which must be taken in consultation with their line manager.\\nSick Leave: If an employee is unable to come to work due to illness, they should notify their line manager as soon as possible and provide a doctor's note if they are absent for more than three days.\\nWorking from Home: Employees may work from home if they have a good reason and have obtained approval from their line manager.\\nParental Leave: Employees are entitled to parental leave if they have worked at Omnicentra for at least one year.\\n\\nOmnicentra provides employees with laptops and any necessary software. If an employee needs a new laptop or software, they should speak to their line manager. Employees are responsible for maintaining the security of their laptop and software.\\nOmnicentra offers the following benefits:\\n\\xa0\\n\\nHealth Insurance: All full-time employees are eligible for health insurance through BUPA.\\nPension Scheme: Omnicentra offers a company pension scheme, which employees can opt into after six months of employment.\\nOther Perks: Employees receive discounts on gym memberships and can take advantage of regular social events and team building activities.\\n\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = generate_context_array(similarities)\n",
    "context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### GPT-generated Prompt response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "response, messages = generate_gpt_chat_response(rand_query, context, \"HR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Yes, there is a probationary period for new employees at Omnicentra. The probationary period is typically six months, during which time the employee's performance will be evaluated to determine if they are a good fit for the company. During this period, the employee will receive feedback and support from their line manager to help them succeed in their role.\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Clear Pinecone DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "PINECONE_API_KEY=os.environ[\"PINECONE_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Initialise pinecone client with valid API key and environment\n",
    "import pinecone\n",
    "\n",
    "pinecone.init(api_key=PINECONE_API_KEY, environment=\"us-west1-gcp-free\")\n",
    "# Connect to the \"Alfred\" index\n",
    "index = pinecone.Index(\"alfred\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe the pincone index\n",
    "index_stats = index.describe_index_stats()\n",
    "# extract the total_vector_count\n",
    "num_vectors = int(index_stats[\"total_vector_count\"])\n",
    "# Use vector count to fetch all vectors in the index\n",
    "ids = [str(x) for x in range(0,38)]\n",
    "index.delete(ids=ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
